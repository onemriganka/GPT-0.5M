{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**GPT-0.5M** by **onemriganka**"
      ],
      "metadata": {
        "id": "gGX62wbYkvWA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UkYH12-qU_E"
      },
      "source": [
        "Step 0: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XF8gNwyKqXp6"
      },
      "outputs": [],
      "source": [
        "!pip install torch requests -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYOSc9cAqg70"
      },
      "source": [
        "Step 1: Base GPT (Character-Level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rzfc9EUKqZ9E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "import time\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------------------\n",
        "# Load dataset from a URL\n",
        "# ----------------------------\n",
        "def load_text(url):\n",
        "    text = requests.get(url).text.lower()\n",
        "    print(\"Dataset length:\", len(text))\n",
        "    return text\n",
        "\n",
        "# ----------------------------\n",
        "# Build character-level vocab\n",
        "# ----------------------------\n",
        "def build_vocab(text):\n",
        "    chars = sorted(list(set(text)))\n",
        "    stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "    itos = {i:ch for i,ch in enumerate(chars)}\n",
        "    vocab_size = len(chars)\n",
        "    print(\"Vocab size:\", vocab_size)\n",
        "    return stoi, itos, vocab_size\n",
        "\n",
        "# ----------------------------\n",
        "# Encode / Decode\n",
        "# ----------------------------\n",
        "def encode(s, stoi): return [stoi[c] for c in s]\n",
        "def decode(l, itos): return ''.join([itos[i] for i in l])\n",
        "\n",
        "# ----------------------------\n",
        "# Positional Encoding\n",
        "# ----------------------------\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# ----------------------------\n",
        "# Transformer Block\n",
        "# ----------------------------\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        attn_output, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
        "        x = self.ln1(x + attn_output)\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.ln2(x + ff_output)\n",
        "        return x\n",
        "\n",
        "# ----------------------------\n",
        "# baseGPT Model\n",
        "# ----------------------------\n",
        "class baseGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=4, ff_dim=256, block_size=128):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = PositionalEncoding(embed_dim, block_size)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x = self.pos_enc(x)\n",
        "        seq_len = x.size(1)\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attn_mask=mask)\n",
        "        logits = self.fc_out(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClxP1P44qsE7"
      },
      "source": [
        "Step 2: Prepare Data & Batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VABqt1k-qn3r"
      },
      "outputs": [],
      "source": [
        "block_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "def get_batch(data):\n",
        "    ix = torch.randint(len(data)-block_size-1, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le2qSv0Tqye9"
      },
      "source": [
        "Step 3: Train the Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UWBIim-6qy3S"
      },
      "outputs": [],
      "source": [
        "def train_model(text, steps=100000, lr=3e-4):\n",
        "    stoi, itos, vocab_size = build_vocab(text)\n",
        "    data = torch.tensor(encode(text, stoi), dtype=torch.long)\n",
        "\n",
        "    model = baseGPT(vocab_size).to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for step in range(steps):\n",
        "        xb, yb = get_batch(data)\n",
        "        logits = model(xb)\n",
        "        loss = loss_fn(logits.view(-1, vocab_size), yb.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(f\"Step {step} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, stoi, itos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvdRDyBMq3d7"
      },
      "source": [
        "Step 4: Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X70l4Qkyq37S"
      },
      "outputs": [],
      "source": [
        "def generate(model, stoi, itos, start=\"The\", max_new_tokens=200):\n",
        "    model.eval()\n",
        "    idx = torch.tensor([stoi.get(c,0) for c in start], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -model.block_size:]\n",
        "        logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "    return ''.join([itos[i] for i in idx[0].tolist()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oECNYjMGq7E6"
      },
      "source": [
        "Step 5: Run Everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZOOD5nxq8fy",
        "outputId": "f38396c5-b69c-4b8c-f95e-1c3c74192b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length: 144696\n",
            "Vocab size: 50\n",
            "Step 0 | Loss: 4.0982\n",
            "Step 1000 | Loss: 1.6565\n",
            "Step 2000 | Loss: 1.3842\n",
            "Step 3000 | Loss: 1.1767\n",
            "Step 4000 | Loss: 1.0614\n",
            "Step 5000 | Loss: 0.9461\n",
            "Step 6000 | Loss: 0.8740\n",
            "Step 7000 | Loss: 0.8208\n",
            "Step 8000 | Loss: 0.7401\n",
            "Step 9000 | Loss: 0.6286\n",
            "Step 10000 | Loss: 0.5860\n",
            "Step 11000 | Loss: 0.5376\n",
            "Step 12000 | Loss: 0.4627\n",
            "Step 13000 | Loss: 0.4181\n",
            "Step 14000 | Loss: 0.3815\n",
            "Step 15000 | Loss: 0.3369\n",
            "Step 16000 | Loss: 0.2991\n",
            "Step 17000 | Loss: 0.2659\n",
            "Step 18000 | Loss: 0.2565\n",
            "Step 19000 | Loss: 0.2409\n",
            "Step 20000 | Loss: 0.2380\n",
            "Step 21000 | Loss: 0.2367\n",
            "Step 22000 | Loss: 0.2265\n",
            "Step 23000 | Loss: 0.2051\n",
            "Step 24000 | Loss: 0.2073\n",
            "Step 25000 | Loss: 0.2064\n",
            "Step 26000 | Loss: 0.2001\n",
            "Step 27000 | Loss: 0.1881\n",
            "Step 28000 | Loss: 0.2043\n",
            "Step 29000 | Loss: 0.1919\n",
            "Step 30000 | Loss: 0.1875\n",
            "Step 31000 | Loss: 0.1784\n",
            "Step 32000 | Loss: 0.1820\n",
            "Step 33000 | Loss: 0.1756\n",
            "Step 34000 | Loss: 0.1717\n",
            "Step 35000 | Loss: 0.1797\n",
            "Step 36000 | Loss: 0.1714\n",
            "Step 37000 | Loss: 0.1632\n",
            "Step 38000 | Loss: 0.1615\n",
            "Step 39000 | Loss: 0.1725\n",
            "Step 40000 | Loss: 0.1688\n",
            "Step 41000 | Loss: 0.1622\n",
            "Step 42000 | Loss: 0.1732\n",
            "Step 43000 | Loss: 0.1755\n",
            "Step 44000 | Loss: 0.1640\n",
            "Step 45000 | Loss: 0.1624\n",
            "Step 46000 | Loss: 0.1621\n",
            "Step 47000 | Loss: 0.1985\n",
            "Step 48000 | Loss: 0.1505\n",
            "Step 49000 | Loss: 0.1490\n",
            "\n",
            "Generated Text:\n",
            "\n",
            "alice, “and we said to the little.\n",
            "\n",
            "“_st,_ i don’t like the look!” said the coonmenly, retire it was gennerally a ridge or furrow in the way wherever she\n",
            "wanted to send the hedgehog to, and, as there\n",
            "weeke cancilled looking thoughtfully at the mushroom for a minute,\n",
            "trying to make out which the sages the twenty at the moment that it was anothing had happened.\n",
            "\n",
            "“how among its you like: you first managed the queen in a deep vingso indeed a\n",
            "shrill: “but it’s no use now, and she story footman for the execute of\n",
            "_court. however, at last the gryphon loked at alice.\n",
            "\n",
            "“_i’ve_ hate_ the world you fly,” said the duchess, and said to alice, “a great girl lap cappper.”\n",
            "\n",
            "“well, if i had our herself to be a great letter,” the mock turtle sounds of him: it was in the didn’t go neap\n",
            "little chin.\n",
            "\n",
            "“i’ve a right to think,” said alice.\n",
            "\n",
            "“it goes on, you know,” the cat, succeed of it had no arme—\n",
            "‘everything’s croquet-toisterd,\n",
            "though story.\n",
            "\n",
            "alice caught it unfolded, and a cratubttly and long close after it, you\n",
            "know.”\n",
            "\n",
            "“i don’t believe it!” puzzled, the same thing as if it spoken at with.”\n",
            "\n",
            "“then i will put on ‘_the explain is tea,” alice said with a lose going alice alone\n",
            "growing to the edge of her sharing a paris, lafter she rant!”\n",
            "but there was no more, and here had been found on each side, and he says\n",
            "now in the high.\n",
            "\n",
            "“i can’t go no bewn knowing her unessaly: “i don’t want to even givef in\n",
            "gaing in at the decided of being seen a long and say ‘come on!’ here,” thought\n",
            "alice!” alice called at once cup.”\n",
            "\n",
            "“what that alq: any shring and vanishing of the court. as as peg it as ever: she had never done\n",
            "finish at all the chimney more than finishing the what she\n",
            "little could go.”\n",
            "\n",
            "“if any one long this the mock turtle you haven’t to have leasp at her earough, “and must be going box in a\n",
            "soger? the hedgene, or at laster; and as he sharp him. the here tried out, and getting that\n",
            "neight-himse should think, and he want on growing.\n",
            "\n",
            "“there’s no such a precopped him,” the queen shrinked in so\n",
            "vauce of one for tearst to be\n",
            "sure! however, everything is queer to-day.”\n",
            "\n",
            "just then she heard something splashing abaty the moral of that is, you know. people through, who\n",
            "looked for round alice, and the three\n",
            "wind, and the pool had toook the knave was went on: realy grave soon at last, mary ann\n",
            "wrote alarm in affter some time after the rest of the general conclusion, that wherever you go\n",
            "to begin the everiting to drawling about, and\n",
            "looking at the gryphon in an impatient tone:\n",
            "“explanations take such a dreadful time.”\n",
            "\n",
            "so alice began telling them her adventures from the time when she first\n",
            "stighing stopped in a deep very leap in the lobster; ind you near the\n",
            "duchess: she thought it must be the right would be a bat?” when she felt\n",
            "“before share thangs looking at alice alone\n",
            "with the gryphon. alice did not quite like the look of the creature,\n",
            "but on the whole she heard a little shriek and a fall, and a crash of broken glass,\n",
            "from which she looked at eggzing every\n",
            "door: you know you’d better for?” she hunfull alice thought, as if the duchess to play\n",
            "croquet with the queen,” and she hutting heehon went on even red have beat\n",
            "frog pressent!” she remuch sudenling very much to-night, i should think!”\n",
            "(dinah was the cat.) “i hope in sight it would be very likely to eat her up in\n",
            "spite of all her coaxing.\n",
            "\n",
            "hatterals. so they set words all excleep, and the white rabbit cried into\n",
            "the gryphon. “i’ll fetch their wretw dinnn other puzzled, but she thrie all sish time to\n",
            "teaste them hight willing at eggs enough to get her head down to them, and thought\n",
            "to herself that hatter. he came in with a things between without\n",
            "herself story and lesson-books!”\n",
            "\n",
            "and so she went on, taking five a little shriek, and alice eager it at all.\n",
            "\n",
            "“then the direction is teacup into the gardeners as a pig, i suppon a ratfilling offended,\n",
            "never”) “—don’t be more what you’re a say to know what it was good\n",
            "manneath, what’s the thing more telling at imposible.\n",
            "\n",
            "“then you know about this business?” the king said to alice.\n",
            "\n",
            "“nothing,” said alice.\n",
            "\n",
            "“you are,” said the king.\n",
            "\n",
            "“then it ought to be number one,” said alice.\n",
            "\n",
            "the king turned punchess tonger!”\n",
            "\n",
            "it was generally a ridge or foe; “that armay that peepped on of the\n",
            "ground gooses the cat: never golden\n",
            "key an honer live at the lone put one house, ‘usid that king it every confusing.”\n",
            "\n",
            "“it isn’t,” said alice: “three inches: are gone, if it majesty minute!” alice thought to\n",
            "herself.\n",
            "\n",
            "“i don’t know only a pack,” she heard herself.\n",
            "\n",
            "“i don’t know in your face,” said the dodo.\n",
            "\n",
            "“kepoon your tellch.”\n",
            "\n",
            "affroming fhave the repeched it had night-forabstching in\n",
            "the back to the little going and sneezing on eagher _some\n",
            "without a little door of the son.\n",
            "\n",
            "“what that alq?” her shapect themselves you shouldere at last, you\n",
            "know, which she looked down at her feet, they seemed to be almost out of\n",
            "sight, they were getting so far off). “oh, my poor little feet, then!” cried the mouse, that he middle of\n",
            "everything that was nibbling at the mushroom\n",
            "(she had k\n"
          ]
        }
      ],
      "source": [
        "# Example: train on Alice in Wonderland\n",
        "url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "text = load_text(url)\n",
        "\n",
        "# Train base GPT\n",
        "start_base = time.time()\n",
        "model, stoi, itos = train_model(text, steps=50000)  # increase steps for better output\n",
        "end_base = time.time()\n",
        "\n",
        "# Generate text\n",
        "print(\"\\nGenerated Text:\\n\")\n",
        "print(generate(model, stoi, itos, start=\"alice\", max_new_tokens=5000))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9_GXS7fvHyy"
      },
      "source": [
        "Now we have a fully functional base character-level GPT.\n",
        "\n",
        "we can paste any dataset link into url and it will train.\n",
        "\n",
        "After training, it will generate text in the style of your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJu5bzEMvMiv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljzHD916rdS7"
      },
      "source": [
        "Step 6: Next Step → Fine-Tuning Q&A\n",
        "\n",
        "we have the base model trained, we can fine-tune it on a Q&A dataset:\n",
        "\n",
        "Prepare a Q&A text dataset.\n",
        "\n",
        "Combine it with your base dataset (or train only on Q&A).\n",
        "\n",
        "Use the same train_model function but with smaller steps and lower learning rate.\n",
        "\n",
        "Generate answers with prompt \"Q: <your question>\\nA:\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p9yRrIYrsvC"
      },
      "source": [
        "Step 1: Prepare Q&A Dataset\n",
        "\n",
        "we can make a small Q&A text file like this (or use your own):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CWmvUKLAq-YS"
      },
      "outputs": [],
      "source": [
        "qa_data = \"\"\"\n",
        "<Q> Who follows the White Rabbit into the rabbit hole?\n",
        "<A> Alice follows the White Rabbit.\n",
        "\n",
        "<Q> What does Alice fall into when she follows the White Rabbit?\n",
        "<A> A deep rabbit hole that leads to Wonderland.\n",
        "\n",
        "<Q> What is written on the bottle Alice drinks from?\n",
        "<A> \"Drink Me.\"\n",
        "\n",
        "<Q> What happens when Alice drinks from the bottle labeled 'Drink Me'?\n",
        "<A> She shrinks to a very small size.\n",
        "\n",
        "<Q> What is written on the cake Alice eats?\n",
        "<A> \"Eat Me.\"\n",
        "\n",
        "<Q> What happens when Alice eats the cake labeled 'Eat Me'?\n",
        "<A> She grows very tall.\n",
        "\n",
        "<Q> Who is always worried about being late?\n",
        "<A> The White Rabbit.\n",
        "\n",
        "<Q> What animal does Alice try to play croquet with?\n",
        "<A> A flamingo as the mallet and hedgehogs as the balls.\n",
        "\n",
        "<Q> Who often says \"Off with their heads!\"?\n",
        "<A> The Queen of Hearts.\n",
        "\n",
        "<Q> Who is the Queen of Hearts married to?\n",
        "<A> The King of Hearts.\n",
        "\n",
        "<Q> What creature gives Alice advice while smoking a hookah?\n",
        "<A> The Caterpillar.\n",
        "\n",
        "<Q> What does the Caterpillar tell Alice about the mushroom?\n",
        "<A> One side makes her grow taller, and the other side makes her smaller.\n",
        "\n",
        "<Q> Who smiles and disappears, leaving only its grin behind?\n",
        "<A> The Cheshire Cat.\n",
        "\n",
        "<Q> Where does Alice meet the Mad Hatter?\n",
        "<A> At the Mad Tea Party.\n",
        "\n",
        "<Q> Who are the guests at the Mad Tea Party?\n",
        "<A> The Mad Hatter, the March Hare, and the Dormouse.\n",
        "\n",
        "<Q> What day do the Mad Hatter and March Hare celebrate endlessly?\n",
        "<A> Their unbirthday.\n",
        "\n",
        "<Q> Who tries to put Alice on trial?\n",
        "<A> The Queen of Hearts and her court.\n",
        "\n",
        "<Q> What crime is Alice accused of during the trial?\n",
        "<A> Stealing tarts.\n",
        "\n",
        "<Q> Who testifies against Alice at the trial?\n",
        "<A> The Mad Hatter and the Cook.\n",
        "\n",
        "<Q> What finally wakes Alice from Wonderland?\n",
        "<A> She wakes up from a dream.\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c6lsPvFsRBg"
      },
      "source": [
        "Combine Datasets to Build a Full Vocabulary\n",
        "\n",
        "Before training/fine-tuning, combine the base dataset + Q&A dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhdJCGeVsL8T",
        "outputId": "650d3424-ca7c-47a2-a238-1d1fb92168c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New vocab size: 54\n"
          ]
        }
      ],
      "source": [
        "# Combine datasets\n",
        "full_text = text.lower() + \"\\n\" + qa_data.lower()\n",
        "\n",
        "# Rebuild vocab\n",
        "chars = sorted(list(set(full_text)))\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "print(\"New vocab size:\", vocab_size)\n",
        "\n",
        "# Encode full text for training/fine-tuning\n",
        "data_full = torch.tensor(encode(full_text, stoi), dtype=torch.long)\n",
        "train_tokens = data_full  # you can use this for fine-tuning too\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KquL_DPur1Ni"
      },
      "source": [
        "Step 2: Encode Q&A Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8nmhd7kNryyy"
      },
      "outputs": [],
      "source": [
        "train_tokens = torch.tensor(encode(qa_data.lower(), stoi), dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhxPdBkLsZfj"
      },
      "source": [
        "Step 3: Fine-Tune Function\n",
        "\n",
        "We’ll reuse the same training loop but with:\n",
        "\n",
        "Lower learning rate (1e-4)\n",
        "\n",
        "Fewer steps (500-1000) for demo\n",
        "\n",
        "Smaller batch if dataset is tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fkvAnqh_r9Kp"
      },
      "outputs": [],
      "source": [
        "def fine_tune_qa(model, data, steps=1000, lr=1e-4):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for step in range(steps):\n",
        "        ix = torch.randint(len(data)-block_size-1, (batch_size,))\n",
        "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits.view(-1, model.fc_out.out_features), y.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(f\"Fine-tuning step {step} | Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4FETuWCszj5"
      },
      "source": [
        "Step 4: Generate Answers\n",
        "\n",
        "Same generate function as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OLYTYyBTs0mQ"
      },
      "outputs": [],
      "source": [
        "def generate_qa(model, prompt, max_new_tokens=100):\n",
        "    return generate(model, stoi, itos, start=prompt, max_new_tokens=max_new_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A3M74Tes5YZ"
      },
      "source": [
        "Step 5: Run Fine-Tuning and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BAgKujWs3lp",
        "outputId": "f174add4-ff7e-447c-8776-5fd6238c68a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning step 0 | Loss: 26.1549\n",
            "Fine-tuning step 1000 | Loss: 0.0620\n",
            "Fine-tuning step 2000 | Loss: 0.0439\n",
            "Fine-tuning step 3000 | Loss: 0.0422\n",
            "Fine-tuning step 4000 | Loss: 0.0382\n",
            "Fine-tuning step 5000 | Loss: 0.0401\n",
            "Fine-tuning step 6000 | Loss: 0.0381\n",
            "Fine-tuning step 7000 | Loss: 0.0391\n",
            "Fine-tuning step 8000 | Loss: 0.0378\n",
            "Fine-tuning step 9000 | Loss: 0.0345\n",
            "Answer 1:\n",
            " \n",
            ": \n",
            "ho is the \n",
            "hite \n",
            "abbit?\n",
            "\n",
            ":<a> the mad hatter and the cook.\n",
            "\n",
            "<q> what finally wakes alice from wonderland?\n",
            "<a> she wakes up fro\n",
            "Answer 2:\n",
            " \n",
            ": \n",
            "hat does \n",
            "lice drink to shrink?\n",
            "\n",
            ":—\n",
            "\n",
            "<q> what creature grows very tall.\n",
            "\n",
            "<q> who is always worried about being late?\n",
            "<a> the white rab\n",
            "Answer 3:\n",
            " \n",
            ": \n",
            "hat happens when \n",
            "lice eats the cake labeled '\n",
            "at \n",
            "e'?\n",
            "\n",
            ":—\n",
            "\n",
            "\n",
            "<q> —\n",
            "\n",
            "\n",
            "<q> where does alice meet the mad hatter?\n",
            "<a> at the mad tea party.\n",
            "\n",
            "<q> who are the gue\n",
            "Answer 4:\n",
            " \n",
            ": \n",
            "ho often says '\n",
            "ff with their heads!'?\n",
            "\n",
            ":<a> the queen of hearts married to?\n",
            "<a> the king of hearts.\n",
            "\n",
            "<q> what creature gives alice advice wh\n",
            "Answer 5:\n",
            " \n",
            ": \n",
            "hat advice does the \n",
            "aterpillar give \n",
            "lice?\n",
            "\n",
            ":<a> stealing tarts.\n",
            "\n",
            "\n",
            "<q> who testifies against alice at the trial?\n",
            "<a> the mad hatter and the cook.\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune base model on Q&A\n",
        "start_finetune = time.time()\n",
        "fine_tune_qa(model, train_tokens, steps=10000, lr=1e-4)\n",
        "end_finetune = time.time()\n",
        "\n",
        "# Test Q&A\n",
        "prompt1 = \"Q: Who is the White Rabbit?\\nA:\"\n",
        "prompt2 = \"Q: What does Alice drink to shrink?\\nA:\"\n",
        "prompt3 = \"Q: What happens when Alice eats the cake labeled 'Eat Me'?\\nA:\"\n",
        "prompt4 = \"Q: Who often says 'Off with their heads!'?\\nA:\"\n",
        "prompt5 = \"Q: What advice does the Caterpillar give Alice?\\nA:\"\n",
        "\n",
        "print(\"Answer 1:\\n\", generate_qa(model, prompt1))\n",
        "print(\"Answer 2:\\n\", generate_qa(model, prompt2))\n",
        "print(\"Answer 3:\\n\", generate_qa(model, prompt3))\n",
        "print(\"Answer 4:\\n\", generate_qa(model, prompt4))\n",
        "print(\"Answer 5:\\n\", generate_qa(model, prompt5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn6aBWPBuMbB"
      },
      "source": [
        "** Model training time and Model parameter count**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOueNNqus7NA",
        "outputId": "a96c2160-8911-4555-9edc-eaa54ffe5247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model training time on google colab T4 GPU: 10.97 Minutes\n",
            "Fine-tuning training time on google colab T4 GPU: 2.21 Minutes\n",
            "Total parameters: 542770 = 0.54 Million\n"
          ]
        }
      ],
      "source": [
        "print(\"Base model training time on google colab T4 GPU:\", round((end_base - start_base) / 60, 2), \"Minutes\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Fine-tuning training time on google colab T4 GPU:\", round((end_finetune - start_finetune) / 60, 2), \"Minutes\")\n",
        "\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total} = {round(total/1e6, 2)} Million\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-0.5M, we’ve built a minimal yet powerful foundation to understand how GPT-style models work — a starting point for further learning, experimentation, and innovation in modern AI.\n",
        "\n",
        "#onemriganka"
      ],
      "metadata": {
        "id": "aqDPU3NQSb9I"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}